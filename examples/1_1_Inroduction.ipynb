{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Fair algorithms are designed to remove unwanted impacts of protected attributes such as gender or ethnicity  on automatic decisions. \n",
    "Early fairness aware machine learning methods viewed fairness from a static point of view and did not consider the delayed impact of decisions on the future.\n",
    "Recently, long term impacts and dynamics of decisions moved into the focus and the long term effects of decisions have been studied. \n",
    "Some of the key insight are, that decisions that appear to be fair from a static point of view may have negative long term impact.   \n",
    "This student project develops a framework to simulate the influence of static decision rules on long term fairness under different assumptions regarding the dynamics of the data generation. The framework is designed to work with the [AIF360](https://aif360.mybluemix.net/) toolkit, in order to provide out of the box access to fairness metrics and algorithms.\n",
    "Data is generated sequentially assuming an impact of previous features and predictions on the current time. Two different assumptions about the data generation are implemented:**\n",
    "\n",
    "**1. Only individuals benefit from positive decisions.**\n",
    "\n",
    "**2. The whole group sharing the protected attribute benefits from positive decisions.**\n",
    "\n",
    "**Depending on the assumptions regarding the data generation static decision rules have different impact on the long term dynamics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fairness in Machine Learning\n",
    "Fairness in static settings is usually evaluated on several data sets  that are public available. For example,  the german credit data set [[GerCred]](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29) or the compas data set [[Compas]](https://github.com/propublica/compas-analysis/). \n",
    "\n",
    "For dynamic fairness there is no such data available and it is not easy to collect it (new data has to change depending on the decisions made). Most works in dynamic or long term fairness therefore make assumptions about the dynamics of the data. These assumptions can be seen as grounded in sociology and social learning. The behavior of an individual is considered to be effect by the environment and not only determined by internal factors.\n",
    "\n",
    "The two key mechanisms assumed to determine dynamics in this project are that individuals learn from experiences and that the social environment serves as role model. For the first factor, learning from experiences, consider a company hiring an employee. After being hired, this employee will increase the skill for this job by experience. If someone was not skilled initially, the right treatment might make this person qualified. An example for social learning for instance is, that children from parents who graduated from university are more likely to graduate themselves [[DZHW]](https://www.dzhw.eu/publikationen/index_html). In this case, the parents are assumed to serve as role model.\n",
    "\n",
    "These assumptions are implemented in form of a sequential data generator sampling new data points given data from previous time steps. The provided framework is meant to be used for estimating the effects of static decision rules on dynamic fairness under two different assumptions regarding the data generation. The assumptions are that either only individuals with  positive predictions benefit from a decision or that the whole group sharing the protected attribute benefits. Details about the data generation are explained in next notebooks. To estimate the effects of a decision rule, a baseline data pipeline provides data generated under the assumption that all individuals in the previous time steps were assigned positive predictions.\n",
    "\n",
    "The baseline data serves as metric here and therefore long term fairness is considered to be measured by the overall number of positive labeled individuals in future. This is not really suitable as fairness measure, as discussed at the end.\n",
    "\n",
    "Problems regarding the data assumptions and the baseline data generation are discussed later, and an improved data generation algorithm is proposed.\n",
    "\n",
    "The jupyter notebooks of chapter two (starting with 2_) describe the data generator and the framework.\n",
    "The next part introduces the notations used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Notation\n",
    "Fair machine learning in this setup is considered as a sequential process at discrete time steps $t$. A superscript $^{(t)}$ will indicate when features at fixed time steps $t$ are referred to. Furthermore, bold capital letters will be used to distinguish between aggregated data and the data of one time step $t$. \n",
    "\n",
    "The following part introduces the general notation at a fixed time steps $t$ (neglecting the superscript $^{(t)}$).\n",
    "\n",
    "Features $$X \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "True labels $$y \\in \\mathbb{R}^{n} $$\n",
    "\n",
    "Predictions $$\\hat{y} \\in \\mathbb{R}^{n} $$\n",
    "\n",
    "Decision function $$d~: X \\rightarrow \\hat{y}$$\n",
    "\n",
    "Error function $$\\mathcal{L}~: \\hat{y} \\times y \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "The optimal decision function $d^*$ is found by minimizing the error function:\n",
    "\n",
    "$$\\min_d \\mathcal{L}(\\hat{y}, y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Static Fairness Setup\n",
    "In fairness aware machine learning, some features $A \\in \\mathbb{R}^{n \\times k} $ are considered to be protected (e.g. describing ethnicity, gender or age). Here, the protected features are not contained in $X$ ($A \\notin X$). The new decision function $d'$ now is a function of two parameters:\n",
    "\n",
    "$$d' : X \\times A \\rightarrow \\hat{y}$$\n",
    "\n",
    "$d'$ must satisfy fairness for the protected features $A$ with respect to some fairness measure $f$. Two example measures are discussed later.\n",
    "\n",
    "The next part briefly scratches, how such a fair decision function $d'$ can be found. However, because no functions are trained in this project it is not discussed in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Finding Fair Decision Rules\n",
    "\n",
    "Three different approaches can be used to compute fair decision rules:\n",
    "\n",
    "#### 1. Pre-Processing:\n",
    "The features $X$ and $A$ are changed such that the resulting data $X_f, A_f$ has no unfairness. \n",
    "\n",
    "#### 2. In-Processing:\n",
    "The learning is adopted, such that the learned decision function $d'$ is fair. This can for instance be achieved by extending the loss function with a penalty term w.r.t. to the desired definition of fairness.\n",
    "\n",
    "#### 3. Post-Processing:\n",
    "A learned decision function $d$ is adjusted such that the output of the new function $d': ~g \\circ d$ is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Measuring Static Fairness\n",
    "Two metrics are exemplary discussed here. There are several ways to describe fairness in terms of a function $f$. In general, the metric $f$ could be any function mapping a subset $ \\mathcal{M} \\in \\{X^{(t)},A^{(t)},Y^{(t)},\\hat{Y}^{(t)} \\}$ to a real value representing the fairness:\n",
    "\n",
    "$$ f: \\mathcal{M} \\in \\{X,A,Y,\\hat{Y}\\} \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "The two metrics described are *disparate impact* and *disparate mistreatment* [[fairmlBook]](https://fairmlbook.org/pdf/classification.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Disparate Impact (or Demographic Parity)\n",
    "Disparate Impact is one of the first metrics considered in fair machine learning. It only considers the acceptance rates and requires that the number of positive labeled individuals between protected and unprotected groups must roughly be the same.\n",
    "\n",
    "Using probability distributions it can be written as:\n",
    "\n",
    "$$P(\\hat{Y}^{(t)} = 1 | A = a_i) = P(\\hat{Y}^{(t)} = 1 | A = a_j) ~~ \\forall i, j $$ \n",
    "\n",
    "Where $1$ is considered to be the positive label.\n",
    "\n",
    "For example, disparate impact states that if a company wants to hire 10 people from two groups, it must hire 5  from each group.\n",
    "This is, however, very restrictive. Therefore a relaxation is the so called $p$-rule. It states that acceptance rates do not need to be equal but only have to satisfy some fraction $p$ of perfect equality.\n",
    "\n",
    "Limits of disparate impact are for instance laziness. A decision function trying to satisfy disparate impact might  accept individuals with a true negative label simply to satisfy disparate impact. Another limitation often mentioned is, that disparate impact can rule out the perfect classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2  Error based Metrics (Equal Opportunity or Separation)\n",
    "Error based metrics consider the true label $Y$.\n",
    "In general, they require the error rates to be minimized. Depending on the scenario one might by interested in overall error rates, false positive/negatives, recall or omissions. \n",
    "\n",
    "The definition as probability distribution would be:\n",
    "\n",
    "$$P(\\hat{Y}^{(t)} = \\hat{y} | Y = y, A = a_i ) = P(\\hat{Y}^{(t)} = \\hat{y} | Y=y, A = a_j) ~~ \\forall i, j $$ \n",
    "\n",
    "One limit if error based metrics is, that a decision maker can trade off error rates between groups. For example, instead of improving the the accuracy for the protected group, the decision maker could simply miss label some individuals of the unprotected group. Compared to disparate impact, error based metrics accept a perfect decision rule since it does not make errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Long Term Fairness Setup\n",
    "The above definition of static fairness is now extended for the sequential process. \n",
    "In this setup, the bold capital letters $\\bf{X}, \\bf{A}, \\bf{Y}, \\bf{\\hat{Y}}$ are aggregated over time with the first dimension representing time steps $t$. The superscript $^{(t)}$ indicates the referred time step. There is only one protected feature and it is considered to be constant over time.\n",
    "\n",
    "$$ Y^{(t)} \\in \\mathbb{R}^{n}$$\n",
    "$$ \\hat{Y}^{(t)} \\in \\mathbb{R}^{n}$$\n",
    "$$ X^{(t)} \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "$$ A^{(t)} = \\bf{A} \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "$$ \\bf{Y} \\in \\mathbb{R}^{t \\times n}$$\n",
    "$$ \\bf{\\hat{Y}} \\in \\mathbb{R}^{t \\times n}$$\n",
    "$$ \\bf{X} \\in \\mathbb{R}^{t \\times n \\times m}$$\n",
    "\n",
    "The definition of the metric $f$ is the same as before (i.e. fairness is only measured at fixed time steps $t$).\n",
    "\n",
    "\n",
    "$$ f: \\mathcal{M} \\in \\{X^{(t)}, A^{(t)}, Y^{(t)},\\hat{Y}^{(t)}\\} \\rightarrow \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limits of Static Metrics\n",
    "The previously mentioned metrics are referred to as observational. They can be expressed as probability distributions over the random variables $X^{(t)}, Y^{(t)}, \\hat{Y}^{(t)}$ and $A^{(t)}$. Some limitations are discussed in the chapter [Inherent limitations of observational criteria](https://fairmlbook.org/pdf/causal.pdf). \n",
    "\n",
    "The limitations addressed here  are related to long term fairness. A decision made at some time point $t$ in this framework has an impact on the future.\n",
    "\n",
    "The data and the true labels $Y$ in the long term decision process are assumed to be distributed proportional to the number of past positive predictions for sub groups $G$ influencing certain individuals.\n",
    "\n",
    "$$P(y_i^{(t)}=1) \\sim \\sum _ {j \\in G} \\sum _ {k=1} ^{n} \\hat{y} _j ^{(t-k)} $$\n",
    "\n",
    "$G$ is a degree of freedom and motivated by the previously mentioned factors such as social learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long term effects are not explicitly considered in static metrics. Nevertheless, they can have impact on long term fairness. It is, however, not directly possible to apply static metrics to the long term data. One could for instance compute a static metric on the aggregated data $\\bf{X, Y, \\hat{Y}, A}$, but this would not measure the desired effects.\n",
    "\n",
    "Consider a perfect decision function (one that makes no errors). From a static point of view, such a function could be seen as fair (i.e. it would satisfy all error based metrics). Under aboves assumption, such a decision rule  would never change the underlining data distribution. The number of individuals from each group labeled negative and positive would stay constant over time. However, if more individuals from the negative labeled group would have been given a chance, the overall number of positive labeled individuals would have increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two assumptions about the influence of decisions are described by the following sets $G$:\n",
    "\n",
    "1. Positive predictions only influence the individual itself, this would be $G=\\{i\\}$.\n",
    "\n",
    "2. The whole group benefits from positive predictions, $G=\\{j | a_j=a_i\\}$.\n",
    "\n",
    "They are discussed in the notebooks of chapter 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Defining Long Term Fairness\n",
    "So far, it was only stated that static fairness metrics do not qualify for measuring long term fairness.\n",
    "This is because the long term view implies other fairness notations beyond static criteria. Since the ability of individuals can improve as consequence of previous decisions, the question is shifted from *is an individual qualified today* to *could the individual be qualified in future*. The most intuitive way to define long term fairness would be, to state that the decision function must maximize the number of positive labeled individuals in future.\n",
    "\n",
    "The proportion of true positive individuals at time $t$ is denoted by:\n",
    "\n",
    "$$ P (Y^{(t)}=1)$$\n",
    "\n",
    "This definition is used around the project, but it is discussed at the end why it might not be the right choice to measure fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Related Work\n",
    "Several frameworks or models have recently been proposed to model or simulate dynamic fairness. Google's fairness gym is the most similar framework compared to this project [[FairnessGym]](https://github.com/google/ml-fairness-gym). It provides different environments to simulate dynamics in data generation and metrics for fairness measurement. The main difference is, that it is built on top of OpenAI Gym, and can be used to run reinforcement learning based agents.\n",
    "\n",
    "In contrast, the  framework in this project simulates fairness with focus on impacts of static decision rules and provides an interface to methods from the AIF360 toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
