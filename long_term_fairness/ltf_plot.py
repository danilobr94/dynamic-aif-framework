"""Visualize long term effect of a classifier

Steps:
    1. Make predictions using decision function on ltf_data set
    2. Evaluate accuracy and fairness ltf_aif on predictions
    3. Sample next generation using sampling function
    4. Repeat step 1

    TODO: pass initial ltf_data set?
"""


import warnings
import matplotlib.pyplot as plt
import numpy as np


class LongTermFairnessPlot:
    """"""

    def __init__(self, data_generator, clf, fairness_metric, update_clf=False, x_lim=None, y_lim=None):
        """

        :param data_generator: (object) must implement sample(X, y, y_hat) and get_label(X) functions
        :param clf: (object) must implement fit(X, X_s, y) and predict(X, X_s) functions
        :param fairness_metric: (function) X, X_s, y, y_hat
        :param update_clf: (bool)
        """
        self._data_generator = data_generator
        self._clf = clf
        self._fairness_metric = fairness_metric

        self._update_clf = update_clf

        self._X_sensitive = None

        self._results = []
        self._X = []
        self._y = []
        self._y_hat = []

        self._baseline_results = []
        self._X_baseline = []
        self._y_baseline = []
        self._y_hat_baseline = []

        self._pos_label = 1
        self._neg_label = 0

        self._pos_class = 1
        self._neg_class = 0

        self._x_lim = x_lim
        self._y_lim = y_lim

    def export_data(self):
        """"""
        raise NotImplemented

    def _fit_clf(self):
        """"""
        X = np.vstack(self._X).squeeze()
        y = np.hstack(self._y).squeeze()

        num_repetitions = np.shape(self._X)[0]
        X_s = np.repeat(self._X_sensitive, num_repetitions)

        self._clf.fit(X, X_s, y)

    def run(self, num_steps):
        """"""
        self.init_data()

        for _i in range(num_steps):
            self.run_generation()
            self._run_baseline_generation()

    def init_data(self):
        """"""
        X_init, X_sens_init, y_init = self._data_generator.sample(None, None, None)
        self._X_sensitive = X_sens_init

        self._X.append(X_init)
        self._y.append(y_init)

        self._fit_clf()
        y_hat_init = self._clf.predict(X_init, X_sens_init)

        self._y_hat.append(y_hat_init)

        self._X_baseline.append(X_init)
        self._y_baseline.append(y_init)
        self._y_hat_baseline.append(y_hat_init)

    def run_generation(self):
        """"""
        X_t, X_sens_t, y_t = self._data_generator.sample(self._X,
                                                         self._y,
                                                         self._y_hat)

        y_hat_t = self._clf.predict(X_t, X_sens_t)

        metric = self._fairness_metric(X_t, X_sens_t, y_t, y_hat_t)

        self._X.append(X_t)
        self._y.append(y_t)
        self._y_hat.append(y_hat_t)

        if self._update_clf:
            self._fit_clf()

        self._results.append(metric)

        return metric

    def _run_baseline_generation(self):
        """Baseline ltf_data and predictions.

        The baseline ltf_data is generated by passing only positive predictions
        """
        y_hat_pos = np.ones(np.shape(self._y_hat_baseline)) * self._pos_label

        X_t_base, X_sens_t_base, y_t_base = self._data_generator.sample(self._X_baseline,
                                                                        self._y_baseline,
                                                                        y_hat_pos)

        y_hat_t_base = self._clf.predict(X_t_base, X_sens_t_base)

        metric = self._fairness_metric(X_t_base, X_sens_t_base, y_t_base, y_hat_t_base)

        self._X_baseline.append(X_t_base)
        self._y_baseline.append(y_t_base)
        self._y_hat_baseline.append(y_hat_t_base)

        self._baseline_results.append(metric)

        return metric

    def plot(self, labels=""):
        """Plots the results over all generations"""
        result_arr = np.asarray(self._results)
        baseline_result_arr = np.asarray(self._baseline_results)
        num_generations, num_metrics = result_arr.shape

        for i in range(num_metrics):
            lbl = "ltf_aif " + str(i) if labels == "" else labels[i]
            plt.plot(range(num_generations), result_arr[:, i], label=lbl)
            plt.plot(range(num_generations),
                     baseline_result_arr[:, i],
                     label="baseline " + lbl,
                     linestyle="--")

        plt.xlabel("Generation")
        plt.legend()
        plt.show()

    def _data_generating_decision_boundary(self, ax, X, num_points=200, cmap="Pastel1"):
        """Plot the data generating decision boundary if the sampling function provides a method to get labels."""

        x1_min, x1_max = X[:, 0].min() - 3, X[:, 0].max() + 3
        x2_min, x2_max = X[:, 1].min() - 3, X[:, 1].max() + 3

        x1_step = (x1_max - x1_min) / num_points
        x2_step = (x2_max - x2_min) / num_points

        xx, yy = np.meshgrid(np.arange(x1_min, x1_max, x1_step),
                             np.arange(x2_min, x2_max, x2_step))

        mesh = np.c_[xx.ravel(), yy.ravel()]

        # TODO: generate protected attribute in plausible way
        X_sens_dummy = np.zeros(np.shape(mesh)[0])

        try:
            label_function = self._data_generator._get_label

        except AttributeError:
            print("failed")

        else:
            Z = label_function(mesh, X_sens_dummy)
            Z = Z.reshape(xx.shape)

            ax.contourf(xx, yy, Z, cmap=cmap)

    def _classifier_decision_boundary(self, ax, X, num_points=200, label="decision boundary"):
        """

        Args:
            ax:
            X: 2D, features at one time step t
            num_points:
            label:

        Returns:

        """
        x1_min, x1_max = X[:, 0].min() - 3, X[:, 0].max() + 3
        x2_min, x2_max = X[:, 1].min() - 3, X[:, 1].max() + 3

        x1_step = (x1_max - x1_min) / num_points
        x2_step = (x2_max - x2_min) / num_points

        xx, yy = np.meshgrid(np.arange(x1_min, x1_max, x1_step),
                             np.arange(x2_min, x2_max, x2_step))

        mesh = np.c_[xx.ravel(), yy.ravel()]

        # TODO: generate protected attribute in plausible way
        X_sens_dummy = np.zeros(np.shape(mesh)[0])

        Z = self._clf.predict(mesh, X_sens_dummy)
        Z = Z.reshape(xx.shape)

        CS = ax.contour(xx, yy, Z)
        CS.collections[0].set_label(label)

    def _plot_data(self, ax, X, y_hat, title="", print_stats=False):
        """

        Args:
            ax:
            X:
            y_hat:
            title:
            print_stats:

        Returns:

        TODO: pass X[-1] insetad of X

        """
        # self._classifier_decision_boundary(ax, X, y_hat)

        pos_lbl_mask = y_hat[-1] == self._pos_label
        neg_lbl_mask = y_hat[-1] == self._neg_label

        pos_cls_mask = self._X_sensitive == self._pos_class
        neg_cls_mask = self._X_sensitive == self._neg_class

        pos_lbl_pos_cls = np.logical_and(pos_lbl_mask, pos_cls_mask)
        ax.scatter(X[-1][pos_lbl_pos_cls, 0],
                   X[-1][pos_lbl_pos_cls, 1],
                   label="pos label and class " + str(np.sum(pos_lbl_pos_cls)),
                   marker="x",
                   c="green")

        pos_lbl_neg_cls = np.logical_and(pos_lbl_mask, neg_cls_mask)
        ax.scatter(X[-1][pos_lbl_neg_cls, 0],
                   X[-1][pos_lbl_neg_cls, 1],
                   label="pos label and neg class " + str(np.sum(pos_lbl_neg_cls)),
                   marker="o",
                   c="darkgreen")

        neg_lbl_pos_cls = np.logical_and(neg_lbl_mask, pos_cls_mask)
        ax.scatter(X[-1][neg_lbl_pos_cls, 0],
                   X[-1][neg_lbl_pos_cls, 1],
                   label="neg label and pos class " + str(np.sum(neg_lbl_pos_cls)),
                   marker="o",
                   c="darkred")

        neg_lbl_neg_cls = np.logical_and(neg_lbl_mask, neg_cls_mask)
        ax.scatter(X[-1][neg_lbl_neg_cls, 0],
                   X[-1][neg_lbl_neg_cls, 1],
                   label="neg label and class " + str(np.sum(neg_lbl_neg_cls)),
                   marker="x",
                   c="red")

        if print_stats:
            txt = "number of positive labels:" + str(np.sum(pos_lbl_mask)) + \
                  "\nnumber of negative labels: " + str(np.sum(neg_lbl_mask)) + "\n" + \
                  "\npositive label positive class: " + str(np.sum(pos_lbl_pos_cls)) + \
                  "\npositive label negative class: " + str(np.sum(pos_lbl_neg_cls)) + \
                  "\nnegative label positive class: " + str(np.sum(neg_lbl_pos_cls)) + \
                  "\nnegative label negative class: " + str(np.sum(neg_lbl_neg_cls))

            print(txt)

        txt = "num positive labels :" + str(np.sum(pos_lbl_mask)) + \
              "\nnum negative labels: " + str(np.sum(neg_lbl_mask)) + "\n"

        if self._x_lim is not None and self._y_lim is not None:
            ax.set_xlim(self._x_lim)
            ax.set_ylim(self._y_lim)

        ax.set_title(title + "\n" + txt)

    def plot_generation(self):
        """Plot ltf_data points of generation"""

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        self._classifier_decision_boundary(ax1, self._X[-1])
        self._classifier_decision_boundary(ax2, self._X_baseline[-1])

        self._data_generating_decision_boundary(ax1, self._X[-1])
        self._data_generating_decision_boundary(ax2, self._X_baseline[-1])

        self._plot_data(ax1, self._X, self._y_hat, "true ltf_data")
        self._plot_data(ax2, self._X_baseline, self._y_hat_baseline, "baseline ltf_data")

        fig.suptitle("Generation " + str(len(self._y_hat)-1))

        ax1.legend()
        ax2.legend()

        plt.show()



