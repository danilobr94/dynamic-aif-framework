{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from methods.data.individual_data_generator import DataGenerator as IndDataGen\n",
    "from methods.data.individual_data_generator_simple import DataGenerator as IndDataGenSim\n",
    "from methods.data.group_data_generator import DataGenerator as GrpDataGen\n",
    "from methods.long_term_fairness import LongTermFairnessPlot\n",
    "from methods.aif360.longterm_aif import AifLongTermMetric, AifLongTermPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness in Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Term Fairness Setup\n",
    "The decision function $d(X^{(t)}, A)$ is a function of the features $X^{(t)}$ at time step $t$, and the protected attributes $A$ (constant over time). The superscript $^{(t)}$ indicates that the features at time step $t$ are meant.\n",
    "\n",
    "$$ Y^{(t)} \\in \\mathbb{R}^{m}$$\n",
    "$$ \\hat{Y}^{(t)} \\in \\mathbb{R}^{m}$$\n",
    "$$A^{(t)} = A \\in \\mathbb{R}^{m}$$\n",
    "$$X^{(t)} \\in \\mathbb{R}^{n \\times m}$$\n",
    "$$X\\in \\mathbb{R}^{t \\times n \\times m}$$\n",
    "\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparate Impact\n",
    "Only considers the acceptance rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error based Metrics\n",
    "Also consider the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of Observational Metrics\n",
    "The assumption is, that a decision made by the decision maker has a future impact on the true label of each individual:\n",
    "\n",
    "$$P([y_i^{(t)}=c]) \\sim \\sum _ {j \\in G} \\sum _ {i=1} ^{n} \\hat{y} _j ^{(t-n)} $$\n",
    "\n",
    "Here,  $G$ is a undefined subset of individuals. The group $G$ can be assigned arbitrary. Two examples are $G_s=\\{i\\}$ and $G_g=\\{j | a_j=a_i\\}$. $G_s$ describes the case, when the positive label only depends on previous predictions of the individual itself and $G_g$ when the positive label depends on positive predictions for all individuals of the same group. Another possible case would be $G_N =\\{j | x_j \\sim x_i\\}$. This would mean, that the probability for a positive label depends on the predictions for individuals with similar features.\n",
    "\n",
    "Under such an assumption, observational criteria might fail to yield optimal solutions. Although they still provide sufficient accuracy in the short term (at the current time step) they might not unleash the full potential of each individual after some generations. This topic is more discussed in the notebooks for each data generation process. \n",
    "\n",
    "This long term view of fairness also implies other fairness notations beyond observational criteria. The question is shifted from \"is an individual qualified for a positive label\" to \"could the individual be made qualified\". This view can be seen as related or as a subtype of causal fairness criteria as discussed in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Fairness\n",
    "Causal fairness notations ask \"why\" individuals are labeled as they are. A label is considered as unfair if a protected attribute has an undesired effect on the decision. What is considered as undesired is a degree of freedom.\n",
    "\n",
    "The relationship between variables can be visualized with cause effect graphs.\n",
    "\n",
    "$$TODO$$\n",
    "\n",
    "In the long term framework the only cause that is considered as unfair is, if previous decision have impact on future qualifications. This can be motivated\n",
    "\n",
    "$$TODO$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Term Fairness Metric\n",
    "Long term fairness as described here and decision maker utility (usually accuracy) are kind of parallel notations.\n",
    "The decision maker must accept some false decision in order to achieve long term fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Long Term Decision Function\n",
    "The framework was used here to visualize the effects of different decision functions in a long term scenario. It is, however, designed to provide a possibility to also train a decision function learning the long term relationship...    \n",
    "\n",
    "A possible extension of the fairness in ML setup could be to extend the notation of the decision makers utility by the number of qualified individuals. I.e. the decision maker does not only want to correctly label all qualified candidates but also to increase the amount of qualified people.\n",
    "\n",
    "$$U \\sim ...$$\n",
    "\n",
    "A nice side effect of this reformulation is the interpretation, that the decision maker now has an actual interest in achieving fairness as opposed to the previous notation where the utility is maximized under some (maybe) annoying fairness constraint. Note, that this does not necessarily has any impact on the solution but is simply a warmer way of formulating the problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif360",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
